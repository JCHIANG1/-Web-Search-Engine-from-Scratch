{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "from html.parser import HTMLParser\n",
    "from urllib.request import Request,urlopen\n",
    "from urllib import parse\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import SoupStrainer\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions applied in the crawler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function is used for obtaining all the tags in html file that used to describe the webpage\n",
    "\n",
    "def drink_soup(link,SoupStrainer):\n",
    "    html_page = urlopen(link).read()\n",
    "    soup = BeautifulSoup(html_page,'lxml',parse_only=SoupStrainer)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the function is used to determine whether the web page is in \"html\" page\n",
    "\n",
    "def doctype(soup):\n",
    "    items = [item for item in soup.contents if isinstance(item, bs4.Doctype)]\n",
    "    try:\n",
    "        if items[0] != \"html\":\n",
    "            return None\n",
    "        else:\n",
    "            return \"html\"\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some links may have different format but all lead to the same final destination, therefore used this function to obtain the link\n",
    "#for final destination\n",
    "#ex.https://www.depaul.edu:443/Pages/default.aspx and http://www.depaul.edu/Pages/default.aspx\n",
    "\n",
    "def redirect_url(link):\n",
    "    try: \n",
    "        link=urlopen(link).geturl()\n",
    "    except:\n",
    "        redirect=requests.get(link)\n",
    "        link=redirect.url\n",
    "    else: \n",
    "        link\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the function is for obtaining unique and normalized link during bfs crawling\n",
    "\n",
    "def obtain_links(start,soup):\n",
    "    start=redirect_url(start)\n",
    "        \n",
    "    collected_link=[]\n",
    "    links=soup.findAll('a')\n",
    "    for link in links:\n",
    "        attrs=link.attrs\n",
    "        if 'href' in attrs:\n",
    "            if attrs['href']==\"\":\n",
    "                continue\n",
    "                \n",
    "            #normalized the link\n",
    "            elif attrs['href'][0]=='/':\n",
    "                l=parse.urljoin(start,attrs['href'])\n",
    "                if \"#\" in l:\n",
    "                    l=l.split(\"#\")[0]\n",
    "\n",
    "                if \"events.depaul.edu/calendar\" in l:\n",
    "                    l=l[:(l.find('calendar')+8)]\n",
    "\n",
    "                if l not in collected_link:\n",
    "                    collected_link.append(l)\n",
    "\n",
    "            #make sure the link is in depaul domain\n",
    "            elif attrs['href'][0]=='h':\n",
    "                if \"depaul.edu\" in attrs['href']:\n",
    "                    l=attrs['href']\n",
    "                    \n",
    "                    if \"#\" in l:\n",
    "                        l=l.split(\"#\")[0]\n",
    "\n",
    "                    if \"events.depaul.edu/calendar\" in l:\n",
    "                        l=l[:(l.find('calendar')+8)]\n",
    "\n",
    "                    if l not in collected_link:\n",
    "                        collected_link.append(l)           \n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    collected_link2=[]\n",
    "    for i in collected_link:\n",
    "        new_link=redirect_url(i)\n",
    "        if \"depaul.edu:443\" in new_link or \"https://sts.is.depaul.edu\" in new_link:\n",
    "            continue\n",
    "        elif new_link not in collected_link2:\n",
    "            collected_link2.append(new_link)    \n",
    "    \n",
    "    if 'https://www.depaul.edu/Pages/default.aspx' in collected_link2:\n",
    "        collected_link2.remove('https://www.depaul.edu/Pages/default.aspx')\n",
    "    \n",
    "    return collected_link2\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the function that obtains the text of the web page given the html tags of the web page\n",
    "\n",
    "def obtain_HTML_text(soup):\n",
    "    text_hold=[]\n",
    "    \n",
    "    if \"ul\" in np.unique([tag.name for tag in soup.find_all()]).tolist():\n",
    "        menu_tags=soup.find_all(\"ul\")[:2]\n",
    "        collect=[]\n",
    "        for tag in menu_tags:\n",
    "            collect.append(tag.find_all(\"a\"))\n",
    "        #formating\n",
    "        clean=[]\n",
    "        for i2 in range(len(collect)):\n",
    "            for item in collect[i2]:\n",
    "                clean.append(item)\n",
    "        \n",
    "    for i in np.unique([tag.name for tag in soup.find_all()]).tolist():\n",
    "        if i not in [\"ul\",'h6','h5','span','label','li','style', 'script', 'meta', '[document]','div','br','font','figure','footer','form','img','input','nav','tr','video']:\n",
    "            if i == \"a\":\n",
    "                tags=soup.find_all(i,{\"rel\":None,\"class\":None,\"data-category-slug\":None,\"data-area\":None,\"aria-label\":None,\"style\":None})\n",
    "                try: \n",
    "                    no_menu=[]\n",
    "                    for i3 in set(tags):\n",
    "                        if i3 not in set(clean):\n",
    "                            no_menu.append(i3)\n",
    "                    tags=no_menu\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "                for t in tags:\n",
    "                    if 'href' in t.attrs.keys():\n",
    "                        if \"#\" in t[\"href\"]:\n",
    "                            continue\n",
    "                        if \"twitter\" in t[\"href\"]:\n",
    "                            continue\n",
    "                        if \"facebook\" in t[\"href\"]:\n",
    "                            continue\n",
    "                        if \"wordpress.org\" in t[\"href\"]:\n",
    "                            continue\n",
    "                        if \"elegantthemes.com\" in t[\"href\"]:\n",
    "                            continue\n",
    "                        if t.string != None:\n",
    "                            text_hold.append(str(t.string))\n",
    "                            \n",
    "            else:\n",
    "                for t2 in set(soup.find_all(i)):\n",
    "                    if t2.string != None:\n",
    "                        text_hold.append(str(t2.string))\n",
    "    return text_hold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The crawler function that applies breadth first  crawling strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The crawler function that applies bfs crawling strategy\n",
    "\n",
    "def bfs_search(start,max_page):\n",
    "    \n",
    "    start=redirect_url(start)\n",
    "    \n",
    "    text={}\n",
    "    \n",
    "    # the root page\n",
    "    root_soup=drink_soup(start,None)\n",
    "    \n",
    "    #--text--#collect text for the root soup\n",
    "    text[start]=obtain_HTML_text(root_soup)\n",
    "    \n",
    "    #--link--# collect links in the first depth / all the links in the root page\n",
    "    neighbours_links=obtain_links(start,root_soup)\n",
    "     \n",
    "    \n",
    "    explored_links=[]\n",
    "    explored_links.append(start)\n",
    "    print(\"link#,appended/text_crawled\",len(explored_links))\n",
    "\n",
    "    for node in neighbours_links:\n",
    "        try:\n",
    "            if node not in explored_links:\n",
    "                try:\n",
    "                    soup=drink_soup(node,None)\n",
    "                except:\n",
    "                    try:\n",
    "                        req = Request(node, headers={'User-Agent': 'Mozilla/5.0'}) #the code prevent block for web scraping\n",
    "                        soup=drink_soup(req,None)\n",
    "                    except:\n",
    "                        print(\"fail to open:\",node)\n",
    "                        continue\n",
    "                if len(np.unique(neighbours_links)) < max_page+1000: #in case some page node fail to open\n",
    "                    #make sure all the pages are html\n",
    "                    if doctype(soup)==\"html\":\n",
    "                        explored_links.append(node)\n",
    "                        text[node]=obtain_HTML_text(soup)\n",
    "                        print(\"link#:\",len(explored_links))\n",
    "\n",
    "                        #collect breadth search link \n",
    "                        try: \n",
    "                            l_lst=obtain_links(node,soup)\n",
    "                            for i in l_lst:\n",
    "                                link=redirect_url(i)\n",
    "                                if link not in explored_links:\n",
    "                                    neighbours_links.append(link)\n",
    "                                    print(\"Unique link in N\",len(np.unique(neighbours_links)))\n",
    "                        except:\n",
    "                            explored_links.pop(explored_links.index(node))\n",
    "                            continue\n",
    "\n",
    "                else:\n",
    "                    if len(explored_links) < max_page+50:  # obtain extra data incase some situation happen\n",
    "                        if doctype(soup)==\"html\":\n",
    "                            explored_links.append(node)\n",
    "                            text[node]=obtain_HTML_text(soup)\n",
    "                            print(\"link#:\",len(explored_links))\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "            #early stoping criteria 2, see if we scrap enough page\n",
    "            elif len(explored_links) > max_page+50:\n",
    "                break\n",
    "                \n",
    "        except:\n",
    "            print(\"last node:\",node)\n",
    "            return text,neighbours_links,explored_links\n",
    "            \n",
    "            \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawling 5000 web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=\"https://www.depaul.edu\"\n",
    "max_page=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it will take about 5 hours to crawl \n",
    "start_time = datetime.now()\n",
    "\n",
    "save_dict=bfs_search(start,max_page)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(save_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the crawled web pages into pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('5000_links_texts.pickle', 'wb') as handle:\n",
    "    #pickle.dump(save_dict, handle)#,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anacon]",
   "language": "python",
   "name": "conda-env-Anacon-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
