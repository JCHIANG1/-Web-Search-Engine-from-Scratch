{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Crawler Inverted Index Construction and Relevant Link Retrieval "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instuction\n",
    "Please exucate all the cells in the notebook and navigate your search with these 3 functions: return_top_15_links / term_freq_finder / inverted_index_info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from math import log\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the 5000 web pages to the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('[CSC575 Final Project Rae Chiang pickle file] 5000_links_texts.pickle', 'rb') as handle:\n",
    "    dict_= pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions to clean the text (tokenization /  bigram/ lowercase /remove punctuation /stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()\n",
    "def process_term(term_lst):\n",
    "    collect=[]\n",
    "    for i in term_lst:\n",
    "        l=i.split()\n",
    "        term_lst=i.split()\n",
    "        for term in term_lst:\n",
    "            collect.append(term)\n",
    "                \n",
    "    #some punctuation cleaning after the tokenization \n",
    "    clean_collect=[]\n",
    "    for t in collect:\n",
    "        if \":\" in t:\n",
    "            t=t.rstrip(':')\n",
    "        if \",\" in t:\n",
    "            t=t.rstrip(',')\n",
    "        if \".\" in t:\n",
    "            t=t.rstrip('.')\n",
    "        if \"(\" in t:           #terms like \"(d2l)\"\n",
    "            t=t.lstrip('(')\n",
    "            try:\n",
    "                t=t.rstrip(')')\n",
    "            except:\n",
    "                continue    \n",
    "        #case folding\n",
    "        term=t.lower()\n",
    "        \n",
    "        #stemming \n",
    "        if term not in stopwords.words(\"english\"):\n",
    "            term=stemmer.stem(term)\n",
    "            if term.isalpha():\n",
    "                term=clean_collect.append(term)\n",
    "            else:\n",
    "                if term.split(\"’\")[0] != term: #filter out some puntuation\n",
    "                    term=term.split(\"’\")[0]    #terms like \"you’re\"\n",
    "                    clean_collect.append(term)\n",
    "\n",
    "    \n",
    "    return clean_collect\n",
    "\n",
    "\n",
    "#formating\n",
    "def process_dict(dictionary):\n",
    "    tokenize_dict={}\n",
    "    for link in dictionary.keys():\n",
    "        tokenize_dict[link]=process_term(dictionary[link])\n",
    "        \n",
    "    return tokenize_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverted Index Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#term freq for the corpus\n",
    "clean=process_dict(dict_)\n",
    "clean_term=[term for link in clean.values() for term in link]\n",
    "term_freq=nltk.FreqDist(clean_term)\n",
    "\n",
    "#assign link_id for each link\n",
    "doc_id={}\n",
    "for index,link in enumerate(clean.keys(),1):\n",
    "    doc_id[index]=link\n",
    "\n",
    "    \n",
    "#posting list construction\n",
    "term_freq_inDoc={} #term freq for each link doc\n",
    "for index,link in doc_id.items():\n",
    "    term_freq_inDoc[index]=nltk.FreqDist(clean[link])\n",
    "\n",
    "posting={}\n",
    "for term in term_freq.keys():\n",
    "    doc_freq_collect_P={}\n",
    "    for index in term_freq_inDoc.keys():\n",
    "        if term in term_freq_inDoc[index].keys():\n",
    "            doc_freq_collect_P[index]=term_freq_inDoc[index][term]\n",
    "\n",
    "    posting[term]=doc_freq_collect_P\n",
    "\n",
    "#inverted index construction [total freq/N Doc/ posting lst (doc#/freq)]\n",
    "inverted_index={}\n",
    "for term in term_freq.keys():                    \n",
    "    inverted_index[term]=(term_freq[term],len(posting[term]),posting[term]) #[N Doc / total freq / posting lst (doc#/freq)]\n",
    "    \n",
    "#calculating idf\n",
    "# doc freq for each term \n",
    "idf={}\n",
    "for term in term_freq.keys():\n",
    "    idf[term]=log(len(doc_id)/len(posting[term]))\n",
    "\n",
    "\n",
    "#construct the posting list with tfidf score with ltc schema\n",
    "# the tf here is 1+log(tf)\n",
    "#the log is based 10\n",
    "\n",
    "H_tfidf_posting={}\n",
    "for term in term_freq.keys():\n",
    "    doc_freq_collect_H={}\n",
    "    for index in term_freq_inDoc.keys():\n",
    "        if term in term_freq_inDoc[index].keys():\n",
    "            doc_freq_collect_H[index]=idf[term]*(1+log(term_freq_inDoc[index][term]))\n",
    "            \n",
    "    H_tfidf_posting[term]=doc_freq_collect_H\n",
    "    \n",
    "    \n",
    "#calculate the d lenth for each doc/link\n",
    "d_length={}\n",
    "for doc_index in doc_id.keys(): \n",
    "    for t in H_tfidf_posting.keys():\n",
    "        \n",
    "        # if t appears in the doc or not\n",
    "        if doc_index in H_tfidf_posting[t].keys():\n",
    "            \n",
    "            # if t in the doc then square the weight and accumulate it in the dict  \n",
    "            d_length[doc_index]=+ H_tfidf_posting[t][doc_index]*H_tfidf_posting[t][doc_index]\n",
    "            \n",
    "    # square root the accumulate number to obtain the length\n",
    "    d_length[doc_index]=np.sqrt(d_length[doc_index])\n",
    "\n",
    "\n",
    "# normalized the tfidf score\n",
    "H_tfidf_posting_normalized={}\n",
    "\n",
    "for term in term_freq.keys():\n",
    "    collect={}\n",
    "    for index in H_tfidf_posting[term].keys():\n",
    "        collect[index]=H_tfidf_posting[term][index]/d_length[index]\n",
    "    H_tfidf_posting_normalized[term]=collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter queries to retrieved the top 10 relevant links based on the cosine similarity score (ltc.ltn schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given the query the function will return the most relevant top 15 links with ltn schema\n",
    "\n",
    "def return_top_10_links(query):\n",
    "    term_freq_q=nltk.FreqDist(process_term([query]))\n",
    "    q_times_d={}\n",
    "    for term in term_freq_q.keys():\n",
    "        #make sure term is the corpus inverted index\n",
    "        if term in idf.keys():\n",
    "\n",
    "            #tfidf score for query term\n",
    "            q_tfidf_score=idf[term]*(1+log(term_freq_q[term]))\n",
    "\n",
    "            for doc,score in H_tfidf_posting_normalized[term].items():\n",
    "                if bool(q_times_d.keys()):\n",
    "                    q_times_d[doc]=+score*q_tfidf_score\n",
    "                else: \n",
    "                    q_times_d[doc]=score*q_tfidf_score\n",
    "\n",
    "    # make sure the score is normalized by the query lenth and doc length\n",
    "    for docID, numerator in q_times_d.items():\n",
    "        q_times_d[docID]=numerator\n",
    "    \n",
    "    sort=sorted(q_times_d.items(), key=lambda dic: dic[1],reverse=True)[:10]\n",
    "    link=[]\n",
    "    for i in sort:\n",
    "        link.append([i[0],doc_id[i[0]],i[1]])\n",
    "    print(\"Top 10 retrieved 1inks:\\n (linkID, link,cosine score)\")\n",
    "    \n",
    "    return link\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the linkID to find the term frequency of the link\n",
    "def term_freq_finder(linkID):\n",
    "    return term_freq_inDoc[linkID]\n",
    "\n",
    "# Enter the term to find the inverted index info of the link\n",
    "# The retern information is in the format of (N Doc / total freq {posting lst (doc#/freq)})\n",
    "def inverted_index_info(term):\n",
    "    return inverted_index[term]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure to execute all the cells above before move on to the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercute the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can assign your query to q, q must be a string\n",
    "# for example\n",
    "q=\"lincoln park campus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excute the function to obtain the top 10 retreval results\n",
    "return_top_10_links(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type in the linkID in term_freq_finder function you will be able to find the term frequency in the web page\n",
    "term_freq_finder(205)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type in the term you are interested in to return the inverted index information, the return information is in this format\n",
    "#[total freq/N Doc/ posting lst (doc#/freq)]\n",
    "inverted_index_info(\"campu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anacon]",
   "language": "python",
   "name": "conda-env-Anacon-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
